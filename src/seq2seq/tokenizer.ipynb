{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('/scratch/props_corpus_train.txt', 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3473638"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(corpus):\n",
    "    return (\n",
    "        corpus[i : i + 1000]\n",
    "        for i in range(0, len(corpus), 1000)\n",
    "    )\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "for line in corpus:\n",
    "    words.update(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3133435"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c285388b0c5476b9772dc18617eaf9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'AlbertTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "old_tokenizer = AlbertTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"!Hero | basedOn | Gospel <TSP> !Hero | musicBy | Eddie DeGarmo <TSP> !Hero | subtitle | The Rock Opera\"\n",
    "tokens = old_tokenizer.tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rdf-albert-tokenizer/tokenizer_config.json',\n",
       " 'rdf-albert-tokenizer/special_tokens_map.json',\n",
       " 'rdf-albert-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"rdf-albert-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_tokenizer = SentencePieceUnigramTokenizer()\n",
    "paths = [\"/scratch/props_corpus.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "other_tokenizer.train(files=paths, vocab_size=32000, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<TSP>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "], show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./sentpiece-rdf-tokenizer-unigram.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_tokenizer.save_model(\".\", \"sentpiece-rdf-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=32000, special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<TSP>\", \"<pad>\", \"<mask>\", \"|\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(corpus), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"<s>\")\n",
    "sep_token_id = tokenizer.token_to_id(\"</s>\")\n",
    "tsp_token_id = tokenizer.token_to_id(\"<TSP>\")\n",
    "print(cls_token_id, sep_token_id)\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"<s>:0 $A:0 <TSP>:0\",\n",
    "    pair=f\"<s>:0 $A:0 <TSP>:0 $B:1 </s>:1\",\n",
    "    special_tokens=[(\"<s>\", cls_token_id), (\"</s>\", cls_token_id), (\"<TSP>\", sep_token_id)],\n",
    ")\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Ad', 'olfo', '_', 'Su', 'Ã¡rez', '_', 'Madrid', 'âĢĵ', 'Bar', 'aj', 'as', '_', 'Airport', 'Ġ', '|', 'ĠrunwayLength', 'Ġ', '|', 'Ġ3500', '.', '0', 'Ġ', '<TSP>', 'ĠAdolfo', '_', 'Su', 'Ã¡rez', '_', 'Madrid', 'âĢĵ', 'Bar', 'aj', 'as', '_', 'Airport', 'Ġ', '|', 'Ġlocation', 'Ġ', '|', 'ĠSan', '_', 'Sebast', 'iÃ¡n', '_', 'de', '_', 'los', '_', 'R', 'ey', 'es', '<TSP>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Adolfo_Suárez_Madrid–Barajas_Airport | runwayLength | 3500.0 <TSP> Adolfo_Suárez_Madrid–Barajas_Airport | location | San_Sebastián_de_los_Reyes\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    cls_token=\"<s>\",\n",
    "    sep_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    additional_special_tokens=[\"<TSP>\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rdf-bpe-tokenizer-32k/tokenizer_config.json',\n",
       " 'rdf-bpe-tokenizer-32k/special_tokens_map.json',\n",
       " 'rdf-bpe-tokenizer-32k/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.save_pretrained(\"rdf-bpe-tokenizer-32k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_roberta = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "tok_scratch = AutoTokenizer.from_pretrained(\"rdf-bpe-tokenizer-32k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = open(\"/scratch/props_corpus_train.txt\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = texts[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "roberta_encodings = tok_roberta(texts, truncation=False, padding=False)\n",
    "scratch_encodings = tok_scratch(texts, truncation=False, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_roberta_length = sum(len(e) for e in roberta_encodings[\"input_ids\"]) / len(roberta_encodings[\"input_ids\"])\n",
    "mean_scratch_length = sum(len(e) for e in scratch_encodings[\"input_ids\"]) / len(scratch_encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184.527, 188.479)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 16k \n",
    "mean_roberta_length, mean_scratch_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184.527, 178.989)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 32k \n",
    "mean_roberta_length, mean_scratch_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tok_scratch.encode(\"Adolfo_Suárez_Madrid–Barajas_Airport | runwayLength | 3500.0 <TSP> Adolfo_Suárez_Madrid–Barajas_Airport | location | San_Sebastián_de_los_Reyes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_map = [tok_scratch.convert_ids_to_tokens(e) for e in encoding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Ad',\n",
       " 'olfo',\n",
       " '_',\n",
       " 'Su',\n",
       " 'Ã¡rez',\n",
       " '_',\n",
       " 'Madrid',\n",
       " 'âĢĵ',\n",
       " 'Bar',\n",
       " 'aj',\n",
       " 'as',\n",
       " '_',\n",
       " 'Airport',\n",
       " 'Ġ',\n",
       " '|',\n",
       " 'ĠrunwayLength',\n",
       " 'Ġ',\n",
       " '|',\n",
       " 'Ġ3500',\n",
       " '.',\n",
       " '0',\n",
       " 'Ġ',\n",
       " '<TSP>',\n",
       " 'ĠAdolfo',\n",
       " '_',\n",
       " 'Su',\n",
       " 'Ã¡rez',\n",
       " '_',\n",
       " 'Madrid',\n",
       " 'âĢĵ',\n",
       " 'Bar',\n",
       " 'aj',\n",
       " 'as',\n",
       " '_',\n",
       " 'Airport',\n",
       " 'Ġ',\n",
       " '|',\n",
       " 'Ġlocation',\n",
       " 'Ġ',\n",
       " '|',\n",
       " 'ĠSan',\n",
       " '_',\n",
       " 'Sebast',\n",
       " 'iÃ¡n',\n",
       " '_',\n",
       " 'de',\n",
       " '_',\n",
       " 'los',\n",
       " '_',\n",
       " 'R',\n",
       " 'ey',\n",
       " 'es',\n",
       " '</s>']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('textbox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84fc63c8ae42b05f54f8c8e4c73411ce0404f059987aac7c448c556c45688d5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
