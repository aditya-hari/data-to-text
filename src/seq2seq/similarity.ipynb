{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random \n",
    "import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
    "sbert_model = AutoModel.from_pretrained(\"sentence-transformers/all-distilroberta-v1\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in sbert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        self.margin = 0.5\n",
    "    \n",
    "    def forward(self, x1, x2, y):\n",
    "        cos_sim = self.cos(x1, x2)\n",
    "        loss = torch.mean((1 - y) * torch.pow(cos_sim, 2) + y * torch.pow(torch.nn.functional.relu(self.margin - cos_sim), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=\"/scratch/checkpoint-400000/\", tokenizer=\"roberta-base\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.5401159524917603, 'token': 13561, 'token_str': ' id', 'sequence': 'Adirondack_Regional_Airport | id | 507'}, {'score': 0.1888379156589508, 'token': 2148, 'token_str': ' capacity', 'sequence': 'Adirondack_Regional_Airport | capacity | 507'}, {'score': 0.06303160637617111, 'token': 25361, 'token_str': ' elevation', 'sequence': 'Adirondack_Regional_Airport | elevation | 507'}, {'score': 0.05397500842809677, 'token': 28697, 'token_str': ' homepage', 'sequence': 'Adirondack_Regional_Airport | homepage | 507'}, {'score': 0.019332336261868477, 'token': 3260, 'token_str': ' code', 'sequence': 'Adirondack_Regional_Airport | code | 507'}]\n"
     ]
    }
   ],
   "source": [
    "result = fill_mask(\"Adirondack_Regional_Airport | <mask> | 507\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.4451245069503784, 'token': 24435, 'token_str': ' Lisbon', 'sequence': 'Brazil | capital | Lisbon'}, {'score': 0.4019171893596649, 'token': 2910, 'token_str': ' Brazil', 'sequence': 'Brazil | capital | Brazil'}, {'score': 0.04149125516414642, 'token': 20698, 'token_str': ' Janeiro', 'sequence': 'Brazil | capital | Janeiro'}, {'score': 0.012677792459726334, 'token': 5716, 'token_str': ' Rio', 'sequence': 'Brazil | capital | Rio'}, {'score': 0.007810568902641535, 'token': 8947, 'token_str': ' Rome', 'sequence': 'Brazil | capital | Rome'}]\n"
     ]
    }
   ],
   "source": [
    "result = fill_mask(\"Brazil | capital | <mask>\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.17329764366149902, 'token': 26698, 'token_str': 'England', 'sequence': 'England | league | English Premier League'}, {'score': 0.02372707426548004, 'token': 48772, 'token_str': 'Offset', 'sequence': 'Offset | league | English Premier League'}, {'score': 0.017689764499664307, 'token': 46525, 'token_str': 'Title', 'sequence': 'Title | league | English Premier League'}, {'score': 0.016985008493065834, 'token': 31661, 'token_str': 'Arsenal', 'sequence': 'Arsenal | league | English Premier League'}, {'score': 0.01610775850713253, 'token': 36571, 'token_str': 'Spain', 'sequence': 'Spain | league | English Premier League'}]\n"
     ]
    }
   ],
   "source": [
    "result = fill_mask(\"<mask> | league | English Premier League\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.14582443237304688, 'token': 22514, 'token_str': ' Belarus', 'sequence': 'Aleksandre_Guruli | team | Belarus'}, {'score': 0.13682851195335388, 'token': 798, 'token_str': ' Russia', 'sequence': 'Aleksandre_Guruli | team | Russia'}, {'score': 0.03605753928422928, 'token': 4644, 'token_str': ' Greece', 'sequence': 'Aleksandre_Guruli | team | Greece'}, {'score': 0.02975333109498024, 'token': 12097, 'token_str': ' Azerbaijan', 'sequence': 'Aleksandre_Guruli | team | Azerbaijan'}, {'score': 0.02689189277589321, 'token': 19848, 'token_str': ' Bulgaria', 'sequence': 'Aleksandre_Guruli | team | Bulgaria'}]\n"
     ]
    }
   ],
   "source": [
    "result = fill_mask(\"Aleksandre_Guruli | team | <mask>\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/checkpoint-400000 were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /scratch/checkpoint-400000 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50266, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "rdf_model = AutoModel.from_pretrained(\"/scratch/checkpoint-400000\").to('cuda')\n",
    "\n",
    "rdf_tokenizer.add_special_tokens({'additional_special_tokens': ['<TSP>']})\n",
    "rdf_model.resize_token_embeddings(len(rdf_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf_model.load_state_dict(torch.load('/scratch/rdf_model_4.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = \"Adirondack_Regional_Airport | elevationAboveTheSeaLevel_(in_metres) | 507\"\n",
    "rdf_tokens = rdf_tokenizer(rdf, return_tensors='pt').to('cuda')\n",
    "rdf_output = rdf_model(**rdf_tokens)\n",
    "rdf_embedding = mean_pooling(rdf_output, rdf_tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Adirondack Regional Airport is 507 metres above sea level.\"\n",
    "sent_tokens = sbert_tokenizer(sent, return_tensors='pt').to('cuda')\n",
    "sent_output = sbert_model(**sent_tokens)\n",
    "sent_embedding = mean_pooling(sent_output, sent_tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0313], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(rdf_embedding, sent_embedding, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src_lines = open(\"/home2/aditya_hari/gsoc/nabu/data/processed_data/eng/train_src\", 'r').readlines()\n",
    "train_tgt_lines = open(\"/home2/aditya_hari/gsoc/nabu/data/processed_data/eng/train_tgt\", 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdfs = {}\n",
    "for src, tgt in zip(train_src_lines, train_tgt_lines):\n",
    "    if(src.strip() not in train_rdfs):\n",
    "        train_rdfs[src.strip()] = []\n",
    "    train_rdfs[src.strip()].append(tgt.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(train_rdfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pairs = []\n",
    "for src in train_rdfs:\n",
    "    for tgt in train_rdfs[src]:\n",
    "        positive_pairs.append((src, tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 125/12796 [00:03<05:18, 39.77it/s]\n",
      "100%|█████████▉| 12791/12796 [03:08<00:00, 76.57it/s]"
     ]
    }
   ],
   "source": [
    "negative_pairs = []\n",
    "pb = tqdm.tqdm(total=len(train_rdfs))\n",
    "for src in train_rdfs:\n",
    "    pb.update(1)\n",
    "    for tgt in train_rdfs[src]:\n",
    "        non_srcs = random.sample(keys, 31)\n",
    "        while src not in non_srcs:\n",
    "            non_srcs = random.sample(keys, 31)\n",
    "        for non_src in non_srcs:\n",
    "            negative_pairs.append((src, random.choice(train_rdfs[non_src])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34352, 1064912)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_pairs), len(negative_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('positive_pairs.txt', 'w')) as f:\n",
    "    for src, tgt in positive_pairs:\n",
    "        f.write(src + '\\t' + tgt + '\\n')\n",
    "\n",
    "with(open('negative_pairs.txt', 'w')) as f:\n",
    "    for src, tgt in negative_pairs:\n",
    "        f.write(src + '\\t' + tgt + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_pointer = 0 \n",
    "training_pairs = []\n",
    "for positive in positive_pairs:\n",
    "    training_pairs.append(positive) \n",
    "    training_pairs.extend(negative_pairs[negative_pointer: negative_pointer + 31])\n",
    "    negative_pointer += 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "for i in range(len(training_pairs)):\n",
    "    if(i % 32 == 0):\n",
    "        train_labels.append(1)\n",
    "    else:\n",
    "        train_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open(\"positive_pairs.txt\", 'w')) as f:\n",
    "    for src, tgt in positive_pairs:\n",
    "        f.write(src + '\\t' + tgt + '\\n')\n",
    "\n",
    "with(open(\"negative_pairs.txt\", 'w')) as f:\n",
    "    for src, tgt in negative_pairs:\n",
    "        f.write(src + '\\t' + tgt + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pairs = [p.split('\\t') for p in open(\"positive_pairs.txt\", 'r').readlines()]\n",
    "negative_pairs = [p.split('\\t') for p in open(\"negative_pairs.txt\", 'r').readlines()]\n",
    "\n",
    "negative_pointer = 0 \n",
    "training_pairs = []\n",
    "for positive in positive_pairs:\n",
    "    training_pairs.append(positive) \n",
    "    training_pairs.extend(negative_pairs[negative_pointer: negative_pointer + 7])\n",
    "    negative_pointer += 31\n",
    "\n",
    "train_labels = []\n",
    "for i in range(len(training_pairs)):\n",
    "    if(i % 8 == 0):\n",
    "        train_labels.append(1)\n",
    "    else:\n",
    "        train_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, pairs, labels, rdf_tokenizer, sbert_tokenizer):\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "        self.rdf_tokenizer = rdf_tokenizer\n",
    "        self.sbert_tokenizer = sbert_tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rdf, text = self.pairs[idx]\n",
    "        rdf_encodings = self.rdf_tokenizer(rdf, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "        text_encodings = self.sbert_tokenizer(text, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "\n",
    "        return rdf_encodings, text_encodings, torch.tensor(self.labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(training_pairs[:33352*8], train_labels[:33352*8], rdf_tokenizer, sbert_tokenizer)\n",
    "eval_dataset = CustomDataset(training_pairs[33352*8:], train_labels[33352*8:], rdf_tokenizer, sbert_tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rdf_model.parameters(), lr=5e-5)\n",
    "loss_fn = ContrastiveLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.01676243171095848:   1%|          | 180/33352 [00:37<1:54:33,  4.83it/s]\n",
      "Epoch: 0, Loss: 0.003210941795259714:   0%|          | 82/33352 [00:13<1:32:02,  6.02it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(rdf_embedding, text_embedding, labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 16\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m pb_train\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, loss\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m     18\u001b[0m pb_train\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/optim/adam.py:412\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 412\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    pb_train = tqdm.tqdm(total=len(train_dataloader))\n",
    "    train_losses = [] \n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        rdf_encodings, text_encodings, labels = batch\n",
    "        rdf_encodings = {key: val.to('cuda').squeeze() for key, val in rdf_encodings.items()}\n",
    "        text_encodings = {key: val.to('cuda').squeeze() for key, val in text_encodings.items()}\n",
    "        rdf_outputs = rdf_model(**rdf_encodings)\n",
    "        text_outputs = sbert_model(**text_encodings)\n",
    "\n",
    "        rdf_embedding = mean_pooling(rdf_outputs, rdf_encodings['attention_mask'])\n",
    "        text_embedding = mean_pooling(text_outputs, text_encodings['attention_mask'])\n",
    "\n",
    "        loss = loss_fn(rdf_embedding, text_embedding, labels.to('cuda'))\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        pb_train.set_description(\"Epoch: {}, Loss: {}\".format(epoch, np.mean(train_losses)))\n",
    "        pb_train.update(1)\n",
    "    pb_train.close()\n",
    "    print(\"Epoch: {}, Train Loss: {}\".format(epoch, np.mean(train_losses)))\n",
    "\n",
    "    pb_eval = tqdm.tqdm(total=len(eval_dataloader))\n",
    "    eval_losses = []\n",
    "    for batch in eval_dataloader:\n",
    "        rdf_encodings, text_encodings, labels = batch\n",
    "        rdf_encodings = {key: val.to('cuda').squeeze() for key, val in rdf_encodings.items()}\n",
    "        text_encodings = {key: val.to('cuda').squeeze() for key, val in text_encodings.items()}\n",
    "        rdf_outputs = rdf_model(**rdf_encodings)\n",
    "        text_outputs = sbert_model(**text_encodings)\n",
    "\n",
    "        rdf_embedding = mean_pooling(rdf_outputs, rdf_encodings['attention_mask'])\n",
    "        text_embedding = mean_pooling(text_outputs, text_encodings['attention_mask'])\n",
    "\n",
    "        loss = loss_fn(rdf_embedding, text_embedding, labels.to('cuda'))\n",
    "        eval_losses.append(loss.item())\n",
    "        pb_eval.set_description(\"Epoch: {}, Loss: {}\".format(epoch, np.mean(eval_losses)))\n",
    "        pb_eval.update(1)\n",
    "    pb_eval.close()\n",
    "    print(\"Epoch: {}, Eval Loss: {}\".format(epoch, np.mean(eval_losses)))\n",
    "    torch.save(rdf_model.state_dict(), '/scratch/rdf_model_{}.pt'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('textbox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84fc63c8ae42b05f54f8c8e4c73411ce0404f059987aac7c448c556c45688d5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
