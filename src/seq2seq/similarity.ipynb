{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel, pipeline \n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import random \n",
    "# import tqdm \n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [i.split('/')[-1] for i in glob.glob('//home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/negatives/*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = [float(i.strip()) for i in open('/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/syntactic/syntactic_pos_scores.txt').readlines()]\n",
    "negative = [float(i.strip()) for i in open('/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/syntactic/syntactic_neg_scores.txt').readlines()]\n",
    "positive_labels = [1 for _ in positive]\n",
    "negative_labels = [0 for _ in negative]\n",
    "merged = positive + negative\n",
    "merged_labels = positive_labels + negative_labels\n",
    "X = np.array(merged).reshape(-1, 1)\n",
    "y = np.array(merged_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41972634855834007 0.13799025000439427\n",
      "0.05598411121275754 0.13244710489478906\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(positive), np.std(positive))\n",
    "print(np.mean(negative), np.std(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1618, 25000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive), len(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fb2b78f7bb0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# print positive and negative distributions on number line \n",
    "plt.scatter(positive, [0 for _ in positive], color='green') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fb2b79366a0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(negative, [0 for _ in negative], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9392140656698474\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/aditya_hari/miniconda3/envs/textbox/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, y)\n",
    "predictions = clf.predict(X)\n",
    "print('Accuracy: ', accuracy_score(y, predictions))\n",
    "print('Precision: ', precision_score(y, predictions))\n",
    "print('Recall: ', recall_score(y, predictions))\n",
    "print('F1: ', f1_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: all-distilroberta-v1_sbert_cont\n",
      "Accuracy: 0.969193778646029\n",
      "F1: 0.7943831494483451\n",
      "Precision: 0.6683544303797468\n",
      "Recall: 0.9789864029666254\n"
     ]
    }
   ],
   "source": [
    "for model_name in ['all-distilroberta-v1_sbert_cont']:\n",
    "    positives = [float(i.strip()) for i in open(f'/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/semantic/positives/{model_name}', 'r').readlines()]\n",
    "    negatives = [float(i.strip()) for i in open(f'/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/semantic/negatives/{model_name}', 'r').readlines()]\n",
    "    positive_syn = [float(i.strip()) for i in open(f'/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/syntactic/syntactic_pos_scores.txt', 'r').readlines()]\n",
    "    negative_syn = [float(i.strip()) for i in open(f'/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/syntactic/syntactic_neg_scores.txt', 'r').readlines()]\n",
    "    positives_2 = list(zip(positives, positive_syn))\n",
    "    negatives_2 = list(zip(negatives, negative_syn))\n",
    "    merged = positives_2 + negatives_2\n",
    "    positive_labels = [1 for i in positives]\n",
    "    negative_labels = [0 for i in negatives]\n",
    "    merged_labels = positive_labels + negative_labels\n",
    "    # X = np.array(merged).reshape(-1, 1)\n",
    "    # y = np.array(merged_labels)\n",
    "    X = np.array(merged)\n",
    "    y = np.array(merged_labels)\n",
    "    clf = SVC(kernel='rbf')\n",
    "    clf.fit(X, y)\n",
    "    #print(clf.coef_)\n",
    "    predictions = clf.predict(X)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y, predictions)}\")\n",
    "    print(f\"F1: {f1_score(y, predictions)}\")\n",
    "    print(f\"Precision: {precision_score(y, predictions)}\")\n",
    "    print(f\"Recall: {recall_score(y, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: all-distilroberta-v1_sbert_cont\n",
      "Accuracy: 0.9692313472086558\n",
      "F1: 0.7974276527331189\n",
      "Precision: 0.6647422680412371\n",
      "Recall: 0.9962917181705809\n"
     ]
    }
   ],
   "source": [
    "for model_name in ['all-distilroberta-v1_sbert_cont']:\n",
    "    positives = [float(i.strip()) for i in open(f'/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/semantic/positives/{model_name}', 'r').readlines()]\n",
    "    negatives = [float(i.strip()) for i in open(f'/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/semantic/negatives/{model_name}', 'r').readlines()]\n",
    "    # positive_syn = [float(i.strip()) for i in open(f'/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/syntactic/syntactic_pos_scores.txt', 'r').readlines()]\n",
    "    # negative_syn = [float(i.strip()) for i in open(f'/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/syntactic/syntactic_neg_scores.txt', 'r').readlines()]\n",
    "    # positives_2 = list(zip(positives, positive_syn))\n",
    "    # negatives_2 = list(zip(negatives, negative_syn))\n",
    "\n",
    "    merged = positives + negatives\n",
    "    positive_labels = [1 for i in positives]\n",
    "    negative_labels = [0 for i in negatives]\n",
    "    merged_labels = positive_labels + negative_labels\n",
    "    # X = np.array(merged).reshape(-1, 1)\n",
    "    # y = np.array(merged_labels)\n",
    "    X = np.array(merged).reshape(-1, 1)\n",
    "    y = np.array(merged_labels)\n",
    "    clf = SVC(kernel='rbf')\n",
    "    clf.fit(X, y)\n",
    "    #print(clf.coef_)\n",
    "    predictions = clf.predict(X)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y, predictions)}\")\n",
    "    print(f\"F1: {f1_score(y, predictions)}\")\n",
    "    print(f\"Precision: {precision_score(y, predictions)}\")\n",
    "    print(f\"Recall: {recall_score(y, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2651653306167852"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2/7.54246415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [1 if i > 0.27 else 0 for i in merged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7953705983747846"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
    "sbert_model = AutoModel.from_pretrained(\"sentence-transformers/all-distilroberta-v1\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in sbert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        self.margin = 0.5\n",
    "    \n",
    "    def forward(self, x1, x2, y):\n",
    "        cos_sim = self.cos(x1, x2)\n",
    "        loss = torch.mean((1 - y) * torch.pow(cos_sim, 2) + y * torch.pow(torch.nn.functional.relu(self.margin - cos_sim), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf_model.load_state_dict(torch.load('/scratch/sbert_cont.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_src = open('/home2/aditya_hari/gsoc/data/processed/en/eval_src', 'r').readlines()\n",
    "eval_tgt = open('/home2/aditya_hari/gsoc/data/processed/en/eval_tgt', 'r').readlines()\n",
    "\n",
    "eval_src_batched = [eval_src[i:i + batch_size] for i in range(0, len(eval_src), batch_size)]\n",
    "eval_tgt_batched = [eval_tgt[i:i + batch_size] for i in range(0, len(eval_tgt), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_pairs = open('/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/negative_pairs.txt').readlines()\n",
    "neg_src = [p.split('\\t')[0] for p in negative_pairs]\n",
    "neg_tgt = [p.split('\\t')[1] for p in negative_pairs]\n",
    "\n",
    "neg_src_batched = [neg_src[i:i + batch_size] for i in range(0, len(neg_src), batch_size)]\n",
    "neg_tgt_batched = [neg_tgt[i:i + batch_size] for i in range(0, len(neg_tgt), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50266, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf_model = AutoModel.from_pretrained(\"sentence-transformers/all-distilroberta-v1\").to('cuda')\n",
    "rdf_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
    "rdf_tokenizer.add_special_tokens({'additional_special_tokens': ['<TSP>']})\n",
    "rdf_model.resize_token_embeddings(len(rdf_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing results before and after pretraining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 147/133114 [00:04<1:09:44, 31.78it/s]\n",
      "  0%|          | 652/133114 [00:11<39:55, 55.31it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     src_output \u001b[38;5;241m=\u001b[39m rdf_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msrc_tokens)\n\u001b[1;32m     10\u001b[0m     src_embedding \u001b[38;5;241m=\u001b[39m mean_pooling(src_output, src_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m     tgt_output \u001b[38;5;241m=\u001b[39m \u001b[43msbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtgt_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     tgt_embedding \u001b[38;5;241m=\u001b[39m mean_pooling(tgt_output, tgt_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     15\u001b[0m similarity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcosine_similarity(src_embedding, tgt_embedding, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:845\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 845\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    848\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    849\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[1;32m    852\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    853\u001b[0m     embedding_output,\n\u001b[1;32m    854\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    862\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    863\u001b[0m )\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:131\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    129\u001b[0m     embeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_embeddings\n\u001b[1;32m    130\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)\n\u001b[0;32m--> 131\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(embeddings)\n\u001b[1;32m    132\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39m, p, training)\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/_VF.py:27\u001b[0m, in \u001b[0;36mVFModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, attr):\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvf, attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "similarities = [] \n",
    "pb = tqdm.tqdm(total=len(neg_src_batched))\n",
    "for src, tgt in zip(neg_src_batched, neg_tgt_batched):\n",
    "    pb.update(1)\n",
    "    src_tokens = rdf_tokenizer(src, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to('cuda')\n",
    "    tgt_tokens = sbert_tokenizer(tgt, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    with(torch.no_grad()):\n",
    "        src_output = rdf_model(**src_tokens)\n",
    "        src_embedding = mean_pooling(src_output, src_tokens['attention_mask'])\n",
    "\n",
    "        tgt_output = sbert_model(**tgt_tokens)\n",
    "        tgt_embedding = mean_pooling(tgt_output, tgt_tokens['attention_mask'].to('cuda'))\n",
    "    \n",
    "    similarity = torch.cosine_similarity(src_embedding, tgt_embedding, dim=1)\n",
    "    similarities.extend(similarity.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 175/203 [00:04<00:00, 36.82it/s]\n",
      " 99%|█████████▊| 200/203 [00:03<00:00, 53.10it/s]"
     ]
    }
   ],
   "source": [
    "similarities = [] \n",
    "pb = tqdm.tqdm(total=len(eval_src_batched))\n",
    "for src, tgt in zip(eval_src_batched, eval_tgt_batched):\n",
    "    pb.update(1)\n",
    "    src_tokens = rdf_tokenizer(src, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to('cuda')\n",
    "    tgt_tokens = sbert_tokenizer(tgt, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    with(torch.no_grad()):\n",
    "        src_output = rdf_model(**src_tokens)\n",
    "        src_embedding = mean_pooling(src_output, src_tokens['attention_mask'])\n",
    "\n",
    "        tgt_output = sbert_model(**tgt_tokens)\n",
    "        tgt_embedding = mean_pooling(tgt_output, tgt_tokens['attention_mask'].to('cuda'))\n",
    "    \n",
    "    similarity = torch.cosine_similarity(src_embedding, tgt_embedding, dim=1)\n",
    "    similarities.extend(similarity.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6130179136217893\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/sbert_pt were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /scratch/sbert_pt and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50266, 768, padding_idx=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf_model = AutoModel.from_pretrained(\"/scratch/sbert_pt\").to('cuda')\n",
    "rdf_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
    "rdf_tokenizer.add_special_tokens({'additional_special_tokens': ['<TSP>']})\n",
    "rdf_model.resize_token_embeddings(len(rdf_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 203/203 [00:09<00:00, 21.67it/s]\n",
      " 99%|█████████▉| 201/203 [00:03<00:00, 51.87it/s]"
     ]
    }
   ],
   "source": [
    "similarities = [] \n",
    "pb = tqdm.tqdm(total=len(eval_src_batched))\n",
    "for src, tgt in zip(eval_src_batched, eval_tgt_batched):\n",
    "    pb.update(1)\n",
    "    src_tokens = rdf_tokenizer(src, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to('cuda')\n",
    "    tgt_tokens = sbert_tokenizer(tgt, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    with(torch.no_grad()):\n",
    "        src_output = rdf_model(**src_tokens)\n",
    "        src_embedding = mean_pooling(src_output, src_tokens['attention_mask'])\n",
    "\n",
    "        tgt_output = sbert_model(**tgt_tokens)\n",
    "        tgt_embedding = mean_pooling(tgt_output, tgt_tokens['attention_mask'].to('cuda'))\n",
    "    \n",
    "    similarity = torch.cosine_similarity(src_embedding, tgt_embedding, dim=1)\n",
    "    similarities.extend(similarity.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2536468929455734\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before and after CT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/sbert_pt were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /scratch/sbert_pt and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf_model = AutoModel.from_pretrained(\"/scratch/sbert_pt\").to('cuda')\n",
    "rdf_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
    "rdf_tokenizer.add_special_tokens({'additional_special_tokens': ['<TSP>']})\n",
    "rdf_model.resize_token_embeddings(len(rdf_tokenizer))\n",
    "rdf_model.load_state_dict(torch.load('/scratch/sbert_cont.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:04<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "source": [
    "similarities = [] \n",
    "pb = tqdm.tqdm(total=len(eval_src_batched))\n",
    "for src, tgt in zip(eval_src_batched, eval_tgt_batched):\n",
    "    pb.update(1)\n",
    "    src_tokens = rdf_tokenizer(src, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to('cuda')\n",
    "    tgt_tokens = sbert_tokenizer(tgt, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    with(torch.no_grad()):\n",
    "        src_output = rdf_model(**src_tokens)\n",
    "        src_embedding = mean_pooling(src_output, src_tokens['attention_mask'])\n",
    "\n",
    "        tgt_output = sbert_model(**tgt_tokens)\n",
    "        tgt_embedding = mean_pooling(tgt_output, tgt_tokens['attention_mask'].to('cuda'))\n",
    "    \n",
    "    similarity = torch.cosine_similarity(src_embedding, tgt_embedding, dim=1)\n",
    "    similarities.extend(similarity.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4009707574312413\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_model = AutoModel.from_pretrained(\"sentence-transformers/all-distilroberta-v1\").to('cuda')\n",
    "rdf_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
    "rdf_tokenizer.add_special_tokens({'additional_special_tokens': ['<TSP>']})\n",
    "rdf_model.resize_token_embeddings(len(rdf_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=\"/scratch/checkpoint-550000/\", tokenizer=\"sentence-transformers/all-distilroberta-v1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.8828219771385193, 'token': 28697, 'token_str': ' homepage', 'sequence': 'Adirondack_Regional_Airport | homepage | 507'}, {'score': 0.04009265452623367, 'token': 13561, 'token_str': ' id', 'sequence': 'Adirondack_Regional_Airport | id | 507'}, {'score': 0.016668234020471573, 'token': 2148, 'token_str': ' capacity', 'sequence': 'Adirondack_Regional_Airport | capacity | 507'}, {'score': 0.011269346810877323, 'token': 6332, 'token_str': ' membership', 'sequence': 'Adirondack_Regional_Airport | membership | 507'}, {'score': 0.009001060388982296, 'token': 43715, 'token_str': ' garrison', 'sequence': 'Adirondack_Regional_Airport | garrison | 507'}]\n"
     ]
    }
   ],
   "source": [
    "result = fill_mask(\"Adirondack_Regional_Airport | <mask> | 507\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.4451245069503784, 'token': 24435, 'token_str': ' Lisbon', 'sequence': 'Brazil | capital | Lisbon'}, {'score': 0.4019171893596649, 'token': 2910, 'token_str': ' Brazil', 'sequence': 'Brazil | capital | Brazil'}, {'score': 0.04149125516414642, 'token': 20698, 'token_str': ' Janeiro', 'sequence': 'Brazil | capital | Janeiro'}, {'score': 0.012677792459726334, 'token': 5716, 'token_str': ' Rio', 'sequence': 'Brazil | capital | Rio'}, {'score': 0.007810568902641535, 'token': 8947, 'token_str': ' Rome', 'sequence': 'Brazil | capital | Rome'}]\n"
     ]
    }
   ],
   "source": [
    "result = fill_mask(\"Brazil | capital | <mask>\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.17329764366149902, 'token': 26698, 'token_str': 'England', 'sequence': 'England | league | English Premier League'}, {'score': 0.02372707426548004, 'token': 48772, 'token_str': 'Offset', 'sequence': 'Offset | league | English Premier League'}, {'score': 0.017689764499664307, 'token': 46525, 'token_str': 'Title', 'sequence': 'Title | league | English Premier League'}, {'score': 0.016985008493065834, 'token': 31661, 'token_str': 'Arsenal', 'sequence': 'Arsenal | league | English Premier League'}, {'score': 0.01610775850713253, 'token': 36571, 'token_str': 'Spain', 'sequence': 'Spain | league | English Premier League'}]\n"
     ]
    }
   ],
   "source": [
    "result = fill_mask(\"<mask> | league | English Premier League\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.14582443237304688, 'token': 22514, 'token_str': ' Belarus', 'sequence': 'Aleksandre_Guruli | team | Belarus'}, {'score': 0.13682851195335388, 'token': 798, 'token_str': ' Russia', 'sequence': 'Aleksandre_Guruli | team | Russia'}, {'score': 0.03605753928422928, 'token': 4644, 'token_str': ' Greece', 'sequence': 'Aleksandre_Guruli | team | Greece'}, {'score': 0.02975333109498024, 'token': 12097, 'token_str': ' Azerbaijan', 'sequence': 'Aleksandre_Guruli | team | Azerbaijan'}, {'score': 0.02689189277589321, 'token': 19848, 'token_str': ' Bulgaria', 'sequence': 'Aleksandre_Guruli | team | Bulgaria'}]\n"
     ]
    }
   ],
   "source": [
    "result = fill_mask(\"Aleksandre_Guruli | team | <mask>\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/checkpoint-400000 were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /scratch/checkpoint-400000 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50266, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "rdf_model = AutoModel.from_pretrained(\"/scratch/checkpoint-400000\").to('cuda')\n",
    "\n",
    "rdf_tokenizer.add_special_tokens({'additional_special_tokens': ['<TSP>']})\n",
    "rdf_model.resize_token_embeddings(len(rdf_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf_model.load_state_dict(torch.load('/scratch/rdf_model_4.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = \"Sachin Tendulkar | occupation | cricketer\"\n",
    "rdf_tokens = rdf_tokenizer(rdf, return_tensors='pt').to('cuda')\n",
    "rdf_output = rdf_model(**rdf_tokens)\n",
    "rdf_embedding = mean_pooling(rdf_output, rdf_tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Ramesh is a cricketer.\"\n",
    "sent_tokens = sbert_tokenizer(sent, return_tensors='pt').to('cuda')\n",
    "sent_output = sbert_model(**sent_tokens)\n",
    "sent_embedding = mean_pooling(sent_output, sent_tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3052], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(rdf_embedding, sent_embedding, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src_lines = open(\"/home2/aditya_hari/gsoc/nabu/data/processed_data/eng/train_src\", 'r').readlines()\n",
    "train_tgt_lines = open(\"/home2/aditya_hari/gsoc/nabu/data/processed_data/eng/train_tgt\", 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdfs = {}\n",
    "for src, tgt in zip(train_src_lines, train_tgt_lines):\n",
    "    if(src.strip() not in train_rdfs):\n",
    "        train_rdfs[src.strip()] = []\n",
    "    train_rdfs[src.strip()].append(tgt.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(train_rdfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pairs = []\n",
    "for src in train_rdfs:\n",
    "    for tgt in train_rdfs[src]:\n",
    "        positive_pairs.append((src, tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 125/12796 [00:03<05:18, 39.77it/s]\n",
      "100%|█████████▉| 12791/12796 [03:08<00:00, 76.57it/s]"
     ]
    }
   ],
   "source": [
    "negative_pairs = []\n",
    "pb = tqdm.tqdm(total=len(train_rdfs))\n",
    "for src in train_rdfs:\n",
    "    pb.update(1)\n",
    "    for tgt in train_rdfs[src]:\n",
    "        non_srcs = random.sample(keys, 31)\n",
    "        while src not in non_srcs:\n",
    "            non_srcs = random.sample(keys, 31)\n",
    "        for non_src in non_srcs:\n",
    "            negative_pairs.append((src, random.choice(train_rdfs[non_src])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34352, 1064912)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_pairs), len(negative_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('positive_pairs.txt', 'w')) as f:\n",
    "    for src, tgt in positive_pairs:\n",
    "        f.write(src + '\\t' + tgt + '\\n')\n",
    "\n",
    "with(open('negative_pairs.txt', 'w')) as f:\n",
    "    for src, tgt in negative_pairs:\n",
    "        f.write(src + '\\t' + tgt + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_pointer = 0 \n",
    "training_pairs = []\n",
    "for positive in positive_pairs:\n",
    "    training_pairs.append(positive) \n",
    "    training_pairs.extend(negative_pairs[negative_pointer: negative_pointer + 31])\n",
    "    negative_pointer += 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "for i in range(len(training_pairs)):\n",
    "    if(i % 32 == 0):\n",
    "        train_labels.append(1)\n",
    "    else:\n",
    "        train_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open(\"positive_pairs.txt\", 'w')) as f:\n",
    "    for src, tgt in positive_pairs:\n",
    "        f.write(src + '\\t' + tgt + '\\n')\n",
    "\n",
    "with(open(\"negative_pairs.txt\", 'w')) as f:\n",
    "    for src, tgt in negative_pairs:\n",
    "        f.write(src + '\\t' + tgt + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pairs = [p.split('\\t') for p in open(\"positive_pairs.txt\", 'r').readlines()]\n",
    "negative_pairs = [p.split('\\t') for p in open(\"negative_pairs.txt\", 'r').readlines()]\n",
    "\n",
    "negative_pointer = 0 \n",
    "training_pairs = []\n",
    "for positive in positive_pairs:\n",
    "    training_pairs.append(positive) \n",
    "    training_pairs.extend(negative_pairs[negative_pointer: negative_pointer + 7])\n",
    "    negative_pointer += 31\n",
    "\n",
    "train_labels = []\n",
    "for i in range(len(training_pairs)):\n",
    "    if(i % 8 == 0):\n",
    "        train_labels.append(1)\n",
    "    else:\n",
    "        train_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, pairs, labels, rdf_tokenizer, sbert_tokenizer):\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "        self.rdf_tokenizer = rdf_tokenizer\n",
    "        self.sbert_tokenizer = sbert_tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rdf, text = self.pairs[idx]\n",
    "        rdf_encodings = self.rdf_tokenizer(rdf, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "        text_encodings = self.sbert_tokenizer(text, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "\n",
    "        return rdf_encodings, text_encodings, torch.tensor(self.labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(training_pairs[:33352*8], train_labels[:33352*8], rdf_tokenizer, sbert_tokenizer)\n",
    "eval_dataset = CustomDataset(training_pairs[33352*8:], train_labels[33352*8:], rdf_tokenizer, sbert_tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rdf_model.parameters(), lr=5e-5)\n",
    "loss_fn = ContrastiveLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.01676243171095848:   1%|          | 180/33352 [00:37<1:54:33,  4.83it/s]\n",
      "Epoch: 0, Loss: 0.003210941795259714:   0%|          | 82/33352 [00:13<1:32:02,  6.02it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(rdf_embedding, text_embedding, labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 16\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m pb_train\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, loss\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m     18\u001b[0m pb_train\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/torch/optim/adam.py:412\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 412\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    pb_train = tqdm.tqdm(total=len(train_dataloader))\n",
    "    train_losses = [] \n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        rdf_encodings, text_encodings, labels = batch\n",
    "        rdf_encodings = {key: val.to('cuda').squeeze() for key, val in rdf_encodings.items()}\n",
    "        text_encodings = {key: val.to('cuda').squeeze() for key, val in text_encodings.items()}\n",
    "        rdf_outputs = rdf_model(**rdf_encodings)\n",
    "        text_outputs = sbert_model(**text_encodings)\n",
    "\n",
    "        rdf_embedding = mean_pooling(rdf_outputs, rdf_encodings['attention_mask'])\n",
    "        text_embedding = mean_pooling(text_outputs, text_encodings['attention_mask'])\n",
    "\n",
    "        loss = loss_fn(rdf_embedding, text_embedding, labels.to('cuda'))\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        pb_train.set_description(\"Epoch: {}, Loss: {}\".format(epoch, np.mean(train_losses)))\n",
    "        pb_train.update(1)\n",
    "    pb_train.close()\n",
    "    print(\"Epoch: {}, Train Loss: {}\".format(epoch, np.mean(train_losses)))\n",
    "\n",
    "    pb_eval = tqdm.tqdm(total=len(eval_dataloader))\n",
    "    eval_losses = []\n",
    "    for batch in eval_dataloader:\n",
    "        rdf_encodings, text_encodings, labels = batch\n",
    "        rdf_encodings = {key: val.to('cuda').squeeze() for key, val in rdf_encodings.items()}\n",
    "        text_encodings = {key: val.to('cuda').squeeze() for key, val in text_encodings.items()}\n",
    "        rdf_outputs = rdf_model(**rdf_encodings)\n",
    "        text_outputs = sbert_model(**text_encodings)\n",
    "\n",
    "        rdf_embedding = mean_pooling(rdf_outputs, rdf_encodings['attention_mask'])\n",
    "        text_embedding = mean_pooling(text_outputs, text_encodings['attention_mask'])\n",
    "\n",
    "        loss = loss_fn(rdf_embedding, text_embedding, labels.to('cuda'))\n",
    "        eval_losses.append(loss.item())\n",
    "        pb_eval.set_description(\"Epoch: {}, Loss: {}\".format(epoch, np.mean(eval_losses)))\n",
    "        pb_eval.update(1)\n",
    "    pb_eval.close()\n",
    "    print(\"Epoch: {}, Eval Loss: {}\".format(epoch, np.mean(eval_losses)))\n",
    "    torch.save(rdf_model.state_dict(), '/scratch/rdf_model_{}.pt'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('textbox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84fc63c8ae42b05f54f8c8e4c73411ce0404f059987aac7c448c556c45688d5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
