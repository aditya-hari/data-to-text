{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.util import ngrams \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "import regex as re \n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np \n",
    "import json \n",
    "import random \n",
    "import spacy \n",
    "import glob \n",
    "import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pairs = [i.split('\\t') for i in open('/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/negative_pairs.txt', 'r').readlines()][:25000]\n",
    "pos_src = open('/home2/aditya_hari/gsoc/data/processed/en/eval_src', 'r').readlines()\n",
    "pos_tgt = open('/home2/aditya_hari/gsoc/data/processed/en/eval_tgt', 'r').readlines()\n",
    "pos_pairs = [[src, tgt] for src, tgt in zip(pos_src, pos_tgt)][:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex pattern to split at camel case\n",
    "pattern = re.compile(r'(?<!^)(?=[A-Z])')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute average TF-IDF similarity between every pair of sentences in two lists of sentences\n",
    "def get_similarity(sent_list1, sent_list2):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    sent_list1 = [' '.join([i for i in pattern.split(sent)]) for sent in sent_list1]\n",
    "    sent_list2 = [' '.join([i for i in pattern.split(sent)]) for sent in sent_list2]\n",
    "    X1 = vectorizer.fit_transform(sent_list1)\n",
    "    X2 = vectorizer.transform(sent_list2)\n",
    "    sim_mat = np.zeros((len(sent_list1), len(sent_list2)))\n",
    "    for i in range(len(sent_list1)):\n",
    "        for j in range(len(sent_list2)):\n",
    "            if(norm(X1[i].toarray()[0])*norm(X2[j].toarray()[0]) == 0):\n",
    "                sim_mat[i][j] = 0\n",
    "            else:\n",
    "                sim_mat[i][j] = dot(X1[i].toarray()[0], X2[j].toarray()[0])/(norm(X1[i].toarray()[0])*norm(X2[j].toarray()[0]))\n",
    "    return sim_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [38:04<00:00, 10.94it/s] \n"
     ]
    }
   ],
   "source": [
    "pb = tqdm.tqdm(total=len(pos_pairs))\n",
    "pos_scores = []\n",
    "for pair in pos_pairs:\n",
    "    pb.update(1)\n",
    "    rdf, sent = pair\n",
    "    rdf_components = rdf.split('|')\n",
    "    prop_str = ' '.join([' '.join(pattern.split(re.sub(r'[^\\w]+', ' ', i))) for i in rdf_components[:]])\n",
    "    sim_mat = get_similarity([sent], [prop_str])\n",
    "    sim_score = sim_mat[0][0]\n",
    "    pos_scores.append(sim_score)\n",
    "    \n",
    "with(open('/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/syntactic/syntactic_pos_scores.txt', 'w')) as f:\n",
    "    f.write('\\n'.join([str(i) for i in pos_scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41972634855834007"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pos_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1618/1618 [00:08<00:00, 182.15it/s]\n",
      "100%|█████████▉| 24974/25000 [00:57<00:00, 448.54it/s]"
     ]
    }
   ],
   "source": [
    "pb = tqdm.tqdm(total=len(neg_pairs))\n",
    "neg_scores = []\n",
    "for pair in neg_pairs:\n",
    "    pb.update(1)\n",
    "    rdf, sent = pair\n",
    "    rdf_components = rdf.split('|')\n",
    "    prop_str = ' '.join([' '.join(pattern.split(re.sub(r'[^\\w]+', ' ', i))) for i in rdf_components[:]])\n",
    "    sim_mat = get_similarity([sent], [prop_str])\n",
    "    sim_score = sim_mat[0][0]\n",
    "    neg_scores.append(sim_score)\n",
    "\n",
    "with(open('/home2/aditya_hari/gsoc/rdf-to-text/src/seq2seq/similarities/syntactic/syntactic_neg_scores.txt', 'w')) as f:\n",
    "    f.write('\\n'.join([str(i) for i in neg_scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05598411121275754"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = tqdm.tqdm(total=len(keys[:100000]))\n",
    "for key in keys[:100000]:\n",
    "    pb.update(1)\n",
    "    try:\n",
    "        all_prop_strs = [] \n",
    "        props_filtered = [prop for prop in candidates[key]['properties'] if('' not in prop)]\n",
    "        for prop in props_filtered:\n",
    "            prop_str = ' '.join([' '.join(pattern.split(re.sub(r'[^\\w]+', ' ', i))) for i in prop])\n",
    "            all_prop_strs.append(prop_str)\n",
    "        if('en_text' in candidates[key] and len(candidates[key]['en_text']) != 0):\n",
    "            en_similarity_mat = get_similarity(all_prop_strs, candidates[key]['en_text'])\n",
    "            en_above_thresh = np.where(en_similarity_mat > 0)\n",
    "            if(len(en_above_thresh[1]) != 0):\n",
    "                # en_retained_props = [[] for _ in range(len(candidates[key]['en_text']))]\n",
    "                # for sent_idx, prop_idx in zip(en_above_thresh[1], en_above_thresh[0]):\n",
    "                #     en_retained_props[sent_idx].append(props_filtered[prop_idx])\n",
    "                if(key not in filtered_candidates):\n",
    "                    filtered_candidates[key] = candidates[key]\n",
    "                # filtered_candidates[key]['en_text'] = candidates[key]['en_text']\n",
    "                # filtered_candidates[key]['en_retained_props'] = en_retained_props\n",
    "                filtered_candidates[key]['en_sim_mat'] = en_similarity_mat.tolist()\n",
    "\n",
    "        if('de_translated' in candidates[key] and len(candidates[key]['de_translated']) != 0):\n",
    "            de_similarity_mat = get_similarity(all_prop_strs, candidates[key]['de_translated'])\n",
    "            de_above_thresh = np.where(de_similarity_mat > 0)\n",
    "            if(len(de_above_thresh[1]) != 0):\n",
    "                # de_retained_props = [[] for _ in range(len(candidates[key]['de_translated']))]\n",
    "                # for sent_idx, prop_idx in zip(de_above_thresh[1], de_above_thresh[0]):\n",
    "                #     de_retained_props[sent_idx].append(props_filtered[prop_idx])\n",
    "                if(key not in filtered_candidates):\n",
    "                    filtered_candidates[key] = candidates[key]\n",
    "                # filtered_candidates[key]['de_translated'] = candidates[key]['de_translated']\n",
    "                # filtered_candidates[key]['de_retained_props'] = de_retained_props\n",
    "                filtered_candidates[key]['de_sim_mat'] = de_similarity_mat.tolist()\n",
    "        \n",
    "        if('ga_translated' in candidates[key] and len(candidates[key]['ga_translated']) != 0):\n",
    "            ga_similarity_mat = get_similarity(all_prop_strs, candidates[key]['ga_translated'])\n",
    "            ga_above_thresh = np.where(ga_similarity_mat > 0)\n",
    "            if(len(ga_above_thresh[1]) != 0):\n",
    "                # ga_retained_props = [[] for _ in range(len(candidates[key]['ga_translated']))]\n",
    "                # for sent_idx, prop_idx in zip(ga_above_thresh[1], ga_above_thresh[0]):\n",
    "                #     ga_retained_props[sent_idx].append(props_filtered[prop_idx])\n",
    "                if(key not in filtered_candidates):\n",
    "                    filtered_candidates[key] = candidates[key]\n",
    "                # filtered_candidates[key]['ga_translated'] = candidates[key]['ga_translated']\n",
    "                # filtered_candidates[key]['ga_retained_props'] = ga_retained_props\n",
    "                filtered_candidates[key]['ga_sim_mat'] = ga_similarity_mat.tolist()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "with(open('props/prop_candidates_0.json', 'w')) as f:\n",
    "    json.dump(filtered_candidates, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('textbox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84fc63c8ae42b05f54f8c8e4c73411ce0404f059987aac7c448c556c45688d5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
