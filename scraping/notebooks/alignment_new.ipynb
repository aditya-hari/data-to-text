{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.util import ngrams \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "import regex as re \n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np \n",
    "import json \n",
    "import random \n",
    "import spacy \n",
    "import glob \n",
    "import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7fbbef5ec6c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('xx_sent_ud_sm')\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {} \n",
    "with open('/scratch/useful/subject_set_labels.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        label_map.update(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_dict = {} \n",
    "with open('/scratch/useful/ontology_props.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        prop_dict.update({line['name']: line['properties']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {} \n",
    "with open('/scratch/useful/mapping_transitive.ttl', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        if(line[0] not in type_dict):\n",
    "            type_dict[line[0]] = set() \n",
    "        type_dict[line[0]].add(line[2].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Agent>',\n",
       " 'DUL.owl#Agent>',\n",
       " 'DUL.owl#SocialPerson>',\n",
       " 'Group>',\n",
       " 'MusicGroup>',\n",
       " 'Organisation>',\n",
       " 'Organization>',\n",
       " 'Q215380>',\n",
       " 'Q24229398>',\n",
       " 'Q43229>',\n",
       " 'owl#Thing>'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_dict['<http://dbpedia.org/resource/!!!>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prop_name(item):\n",
    "    if(\"#literal\" in item):\n",
    "        return item.split('#literal')[0], True\n",
    "    if(item in label_map):\n",
    "        tail_name = label_map[item]\n",
    "    else:\n",
    "        tail_name = re.sub('_', ' ',item.split('/')[-1])\n",
    "    return tail_name, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_types = [\"Place\", \"Person\", \"Organization\", \"Organisation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['ga', 'de', 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 373/1000000 [00:07<5:34:04, 49.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 26258/1000000 [00:04<02:37, 6163.96it/s]\n",
      " 64%|██████▍   | 642660/1000000 [01:30<00:55, 6388.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 642713/1000000 [01:30<00:50, 7091.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    print(lang)\n",
    "    with(open(f'/scratch/useful/abstracts_{lang}.jsonl', 'r')) as f:\n",
    "        pb = tqdm.tqdm(total=1000000)\n",
    "        for i, line in enumerate(f):\n",
    "            pb.update(1)\n",
    "            item = json.loads(line)\n",
    "            rsc = item['resource']\n",
    "            txt = item['text']\n",
    "            found = False \n",
    "\n",
    "            for t in wanted_types:\n",
    "                if(f'{t}>' in type_dict[f'<{rsc}>']):\n",
    "                    found = True \n",
    "            \n",
    "            if(not(found)):\n",
    "                continue  \n",
    "\n",
    "            name = get_prop_name(rsc)[0]\n",
    "            if(name not in candidates):\n",
    "                candidates[name] = {}\n",
    "\n",
    "                properties = [] \n",
    "                fw_props = prop_dict[rsc]['properties']\n",
    "                for prop in fw_props:\n",
    "                    for item in fw_props[prop]:\n",
    "                        item_name = get_prop_name(item)[0]\n",
    "                        properties.append((name, prop, item_name))\n",
    "                \n",
    "                rv_props = prop_dict[rsc]['reverse_properties']\n",
    "                for prop in rv_props:\n",
    "                    for item in rv_props[prop][:3]:\n",
    "                        item_name = get_prop_name(item)[0]\n",
    "                        properties.append((item_name, prop, name))\n",
    "                \n",
    "                candidates[name]['properties'] = properties\n",
    "\n",
    "            if(lang == 'de'):\n",
    "                sents = sent_tokenize(txt, language='german')\n",
    "            else:\n",
    "                sents = sent_tokenize(txt)\n",
    "                \n",
    "            filtered_sents = [] \n",
    "            for sent_ in sents:\n",
    "                # sent_ = sent.text\n",
    "                if(len(sent_.split()) > 5 and len(sent_.split()) < 200):\n",
    "                    filtered_sents.append(sent_)\n",
    "            candidates[name][f'{lang}_text'] = filtered_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = 'filtered_candidates.json'\n",
    "with open(save_name, 'w') as f:\n",
    "    json.dump(candidates, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = json.load(open('filtered_candidates.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(candidates.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"’s-Hertogenbosch [ˌsɛrtoːɣə(n)ˈbɔs] (im allgemeinen Sprachgebrauch Den Bosch [dɛnˈbɔs]; deutsch Herzogenbusch, französisch Bois-le-Duc) ist die Hauptstadt der niederländischen Provinz Noord-Brabant.',\n",
       " 'Die Gemeinde ’s-Hertogenbosch umfasst die Stadt ’s-Hertogenbosch sowie die Dörfer und Ortschaften Bokhoven, Empel, Engelen, Hintham, Kruisstraat, Meerwijk, Orthen, Rosmalen und Maliskamp.',\n",
       " 'Am 1. Januar 2022 lebten laut CBS 156.521 Einwohner in der Gemeinde.',\n",
       " 'Die Stadt ist Sitz des römisch-katholischen Bistums ’s-Hertogenbosch.',\n",
       " 'Die Stadt ist ferner Sitz eines Gerichtes, der Provinzialverwaltung, verschiedener Krankenhäuser und psychiatrischer Anstalten sowie vieler überregional bedeutender Schulen.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[keys[1]]['de_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_de = [] \n",
    "de_files = sorted(glob.glob('/home2/aditya_hari/gsoc/rdf-to-text/scraping/sents/*_translated_sents_de.txt'))\n",
    "for file in de_files:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            all_de.append(line.split('@@@')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ga = []\n",
    "ga_files = sorted(glob.glob('/home2/aditya_hari/gsoc/rdf-to-text/scraping/sents/*_translated_sents_ga.txt'))\n",
    "for file in ga_files:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            all_ga.append(line.split('@@@')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_ptr = 0\n",
    "ga_ptr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in candidates:\n",
    "    if('de_text' in candidates[entity]):\n",
    "        candidates[entity]['de_translated'] = all_de[de_ptr:de_ptr+len(candidates[entity]['de_text'])]\n",
    "        de_ptr += len(candidates[entity]['de_text'])\n",
    "    if('ga_text' in candidates[entity]):\n",
    "        candidates[entity]['ga_translated'] = all_ga[ga_ptr:ga_ptr+len(candidates[entity]['ga_text'])]\n",
    "        ga_ptr += len(candidates[entity]['ga_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"s-Hertogenbosch [ˌsɛrtoːɣə(n)ˈbɔs] (in common parlance Den Bosch [dɛnˈbɔs]; German Herzogenbusch, French Bois-le-Duc) is the capital of the Dutch province of Noord-Brabant.',\n",
       " 'The municipality of s-Hertogenbosch includes the town of s-Hertogenbosch as well as the villages and towns of Bokhoven, Empel, Angels, Hintham, Kruisstraat, Meerwijk, Orten, Rosmalen and Maliskamp.',\n",
       " 'As of January 1, 2022, according to CBS, the municipality had 156,521 inhabitants.',\n",
       " 'The city is the seat of the Roman Catholic Diocese of s-Hertogenbosch.',\n",
       " 'The city is also home to a court, the provincial administration, various hospitals and psychiatric institutions, and many important schools across the region.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[keys[1]]['de_translated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1542708, 1542708, 57677, 57677)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_ptr, len(all_de), ga_ptr, len(all_ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = 'filtered_candidates.json'\n",
    "with open(save_name, 'w') as f:\n",
    "    json.dump(candidates, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = json.load(open('/home2/aditya_hari/gsoc/rdf-to-text/scraping/notebooks/filtered_candidates.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex pattern to split at camel case\n",
    "pattern = re.compile(r'(?<!^)(?=[A-Z])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute average TF-IDF similarity between every pair of sentences in two lists of sentences\n",
    "def get_similarity(sent_list1, sent_list2):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    sent_list1 = [' '.join([i for i in pattern.split(sent)]) for sent in sent_list1]\n",
    "    sent_list2 = [' '.join([i for i in pattern.split(sent)]) for sent in sent_list2]\n",
    "    X1 = vectorizer.fit_transform(sent_list1)\n",
    "    X2 = vectorizer.transform(sent_list2)\n",
    "    sim_mat = np.zeros((len(sent_list1), len(sent_list2)))\n",
    "    for i in range(len(sent_list1)):\n",
    "        for j in range(len(sent_list2)):\n",
    "            if(norm(X1[i].toarray()[0])*norm(X2[j].toarray()[0]) == 0):\n",
    "                sim_mat[i][j] = 0\n",
    "            else:\n",
    "                sim_mat[i][j] = dot(X1[i].toarray()[0], X2[j].toarray()[0])/(norm(X1[i].toarray()[0])*norm(X2[j].toarray()[0]))\n",
    "    return sim_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_candidates = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 329/512471 [00:33<35:48:01,  3.97it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[213], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m         filtered_candidates[key][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde_retained_props\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m de_retained_props\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mga_translated\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m candidates[key] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(candidates[key][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mga_translated\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m     ga_similarity_mat \u001b[38;5;241m=\u001b[39m \u001b[43mget_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_prop_strs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mga_translated\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     ga_above_thresh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(ga_similarity_mat \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.25\u001b[39m)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(ga_above_thresh[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "Cell \u001b[0;32mIn[201], line 14\u001b[0m, in \u001b[0;36mget_similarity\u001b[0;34m(sent_list1, sent_list2)\u001b[0m\n\u001b[1;32m     12\u001b[0m             sim_mat[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m             sim_mat[i][j] \u001b[38;5;241m=\u001b[39m dot(\u001b[43mX1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()[\u001b[38;5;241m0\u001b[39m], X2[j]\u001b[38;5;241m.\u001b[39mtoarray()[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m/\u001b[39m(norm(X1[i]\u001b[38;5;241m.\u001b[39mtoarray()[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39mnorm(X2[j]\u001b[38;5;241m.\u001b[39mtoarray()[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sim_mat\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/scipy/sparse/_index.py:55\u001b[0m, in \u001b[0;36mIndexMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(col, \u001b[39mslice\u001b[39m):\n\u001b[1;32m     54\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_on_1d_array_slice()\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_intXslice(row, col)\n\u001b[1;32m     56\u001b[0m \u001b[39melif\u001b[39;00m col\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     57\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_on_1d_array_slice()\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/scipy/sparse/_csr.py:284\u001b[0m, in \u001b[0;36mcsr_matrix._get_intXslice\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_intXslice\u001b[39m(\u001b[39mself\u001b[39m, row, col):\n\u001b[1;32m    283\u001b[0m     \u001b[39mif\u001b[39;00m col\u001b[39m.\u001b[39mstep \u001b[39min\u001b[39;00m (\u001b[39m1\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 284\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_submatrix(row, col, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    285\u001b[0m     \u001b[39m# TODO: uncomment this once it's faster:\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# return self.getrow(row)._minor_slice(col)\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     M, N \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/scipy/sparse/_compressed.py:814\u001b[0m, in \u001b[0;36m_cs_matrix._get_submatrix\u001b[0;34m(self, major, minor, copy)\u001b[0m\n\u001b[1;32m    810\u001b[0m indptr, indices, data \u001b[39m=\u001b[39m get_csr_submatrix(\n\u001b[1;32m    811\u001b[0m     M, N, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, i0, i1, j0, j1)\n\u001b[1;32m    813\u001b[0m shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap((i1 \u001b[39m-\u001b[39m i0, j1 \u001b[39m-\u001b[39m j0))\n\u001b[0;32m--> 814\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m((data, indices, indptr), shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    815\u001b[0m                       dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/scipy/sparse/_compressed.py:106\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 106\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_format(full_check\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/textbox/lib/python3.8/site-packages/scipy/sparse/_compressed.py:171\u001b[0m, in \u001b[0;36m_cs_matrix.check_format\u001b[0;34m(self, full_check)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr) \u001b[39m!=\u001b[39m major_dim \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    169\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mindex pointer size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) should be (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr), major_dim \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m--> 171\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr[\u001b[39m0\u001b[39;49m] \u001b[39m!=\u001b[39;49m \u001b[39m0\u001b[39;49m):\n\u001b[1;32m    172\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mindex pointer should start with 0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[39m# check index and data arrays\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pb = tqdm.tqdm(total=len(keys))\n",
    "for key in keys:\n",
    "    pb.update(1)\n",
    "    all_prop_strs = [] \n",
    "    props_filtered = [prop for prop in candidates[key]['properties'] if('' not in prop)]\n",
    "    for prop in props_filtered:\n",
    "        prop_str = ' '.join([' '.join(pattern.split(re.sub(r'[^\\w]+', ' ', i))) for i in prop])\n",
    "        all_prop_strs.append(prop_str)\n",
    "    en_similarity_mat = get_similarity(all_prop_strs, candidates[key]['en_text'])\n",
    "    en_above_thresh = np.where(en_similarity_mat > 0.25)\n",
    "    if(len(en_above_thresh[1]) != 0):\n",
    "        en_retained_props = [[] for _ in range(len(candidates[key]['en_text']))]\n",
    "        for sent_idx, prop_idx in zip(en_above_thresh[1], en_above_thresh[0]):\n",
    "            en_retained_props[sent_idx].append(props_filtered[prop_idx])\n",
    "        if(key not in filtered_candidates):\n",
    "            filtered_candidates[key] = {}\n",
    "        filtered_candidates[key]['en_text'] = candidates[key]['en_text']\n",
    "        filtered_candidates[key]['en_retained_props'] = en_retained_props\n",
    "\n",
    "    if('de_translated' in candidates[key] and len(candidates[key]['de_translated']) != 0):\n",
    "        de_similarity_mat = get_similarity(all_prop_strs, candidates[key]['de_translated'])\n",
    "        de_above_thresh = np.where(de_similarity_mat > 0.25)\n",
    "        if(len(de_above_thresh[1]) != 0):\n",
    "            de_retained_props = [[] for _ in range(len(candidates[key]['de_translated']))]\n",
    "            for sent_idx, prop_idx in zip(de_above_thresh[1], de_above_thresh[0]):\n",
    "                de_retained_props[sent_idx].append(props_filtered[prop_idx])\n",
    "            if(key not in filtered_candidates):\n",
    "                filtered_candidates[key] = {}\n",
    "            filtered_candidates[key]['de_translated'] = candidates[key]['de_translated']\n",
    "            filtered_candidates[key]['de_retained_props'] = de_retained_props\n",
    "    \n",
    "    if('ga_translated' in candidates[key] and len(candidates[key]['ga_translated']) != 0):\n",
    "        ga_similarity_mat = get_similarity(all_prop_strs, candidates[key]['ga_translated'])\n",
    "        ga_above_thresh = np.where(ga_similarity_mat > 0.25)\n",
    "        if(len(ga_above_thresh[1]) != 0):\n",
    "            ga_retained_props = [[] for _ in range(len(candidates[key]['ga_translated']))]\n",
    "            for sent_idx, prop_idx in zip(ga_above_thresh[1], ga_above_thresh[0]):\n",
    "                ga_retained_props[sent_idx].append(props_filtered[prop_idx])\n",
    "            if(key not in filtered_candidates):\n",
    "                filtered_candidates[key] = {}\n",
    "            filtered_candidates[key]['ga_translated'] = candidates[key]['ga_translated']\n",
    "            filtered_candidates[key]['ga_retained_props'] = ga_retained_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import glob\n",
    "import numpy as np \n",
    "import regex as re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = {} \n",
    "jsons = glob.glob('/home2/aditya_hari/gsoc/rdf-to-text/scraping/scripts/props/*')\n",
    "for js in jsons:\n",
    "    current_dict = json.load(open(js, 'r'))\n",
    "    merged_dict.update(current_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512297"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(merged_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['properties', 'de_text', 'en_text', 'de_translated', 'en_sim_mat', 'de_sim_mat'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dict[keys[0]].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_candidates = {}\n",
    "for key, value in merged_dict.items(): \n",
    "    if('en_text' in value and 'en_sim_mat' in value):\n",
    "        if(key not in filtered_candidates):\n",
    "            filtered_candidates[key] = {}\n",
    "        filtered_candidates[key]['en_text'] = value['en_text']\n",
    "        en_sim_mat = np.array(value['en_sim_mat'])\n",
    "        en_above_thresh = np.where(en_sim_mat > 0.25)\n",
    "        if(len(en_above_thresh[1]) != 0):\n",
    "            en_retained_props = [[] for _ in range(len(value['en_text']))]\n",
    "            for sent_idx, prop_idx in zip(en_above_thresh[1], en_above_thresh[0]):\n",
    "                if('' not in value['properties'] and value['properties'][prop_idx][0]!=value['properties'][prop_idx][2]):\n",
    "                    en_retained_props[sent_idx].append(value['properties'][prop_idx])\n",
    "            filtered_candidates[key]['en_retained_props'] = en_retained_props\n",
    "    \n",
    "    if('de_text' in value and 'de_sim_mat' in value):\n",
    "        if(key not in filtered_candidates):\n",
    "            filtered_candidates[key] = {}\n",
    "        filtered_candidates[key]['de_text'] = value['de_text']\n",
    "        de_sim_mat = np.array(value['de_sim_mat'])\n",
    "        de_above_thresh = np.where(de_sim_mat > 0.25)\n",
    "        if(len(de_above_thresh[1]) != 0):\n",
    "            de_retained_props = [[] for _ in range(len(value['de_text']))]\n",
    "            for sent_idx, prop_idx in zip(de_above_thresh[1], de_above_thresh[0]):\n",
    "                if('' not in value['properties'] and value['properties'][prop_idx][0]!=value['properties'][prop_idx][2]):\n",
    "                    de_retained_props[sent_idx].append(value['properties'][prop_idx])\n",
    "            filtered_candidates[key]['de_retained_props'] = de_retained_props\n",
    "    \n",
    "    if('ga_text' in value and 'ga_sim_mat' in value):\n",
    "        if(key not in filtered_candidates):\n",
    "            filtered_candidates[key] = {}\n",
    "        filtered_candidates[key]['ga_text'] = value['ga_text']\n",
    "        ga_sim_mat = np.array(value['ga_sim_mat'])\n",
    "        ga_above_thresh = np.where(ga_sim_mat > 0.25)\n",
    "        if(len(ga_above_thresh[1]) != 0):\n",
    "            ga_retained_props = [[] for _ in range(len(value['ga_text']))]\n",
    "            for sent_idx, prop_idx in zip(ga_above_thresh[1], ga_above_thresh[0]):\n",
    "                if('' not in value['properties'] and value['properties'][prop_idx][0]!=value['properties'][prop_idx][2]):\n",
    "                    ga_retained_props[sent_idx].append(value['properties'][prop_idx])\n",
    "            filtered_candidates[key]['ga_retained_props'] = ga_retained_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(filtered_candidates.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_spaces = lambda x: ' '.join(x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_prop_pairs = [] \n",
    "sent_prop_src = [] \n",
    "for key in keys:\n",
    "    if('en_retained_props' in filtered_candidates[key]):\n",
    "        for sent_idx, props in enumerate(filtered_candidates[key]['en_retained_props']):\n",
    "            for prop in props:\n",
    "                sent_prop_pairs.append((filtered_candidates[key]['en_text'][sent_idx], ' | '.join(prop)))\n",
    "                sent_prop_src.append((key, sent_idx, 'en'))\n",
    "    \n",
    "    if('de_retained_props' in filtered_candidates[key]):\n",
    "        for sent_idx, props in enumerate(filtered_candidates[key]['de_retained_props']):\n",
    "            for prop in props:\n",
    "                sent_prop_pairs.append((filtered_candidates[key]['de_text'][sent_idx], ' | '.join(prop)))\n",
    "                sent_prop_src.append((key, sent_idx, 'de'))\n",
    "        \n",
    "    if('ga_retained_props' in filtered_candidates[key]):\n",
    "        for sent_idx, props in enumerate(filtered_candidates[key]['ga_retained_props']):\n",
    "            for prop in props:\n",
    "                sent_prop_pairs.append((filtered_candidates[key]['ga_text'][sent_idx], ' | '.join(prop)))\n",
    "                sent_prop_src.append((key, sent_idx, 'ga'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5608734"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_prop_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Maroua', 0, 'de'),\n",
       " ('\"Maroua ist die Hauptstadt der kamerunischen Region Extrême-Nord und des Departements Diamaré.',\n",
       "  'University of Maroua | city | Maroua'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_prop_src[5], sent_prop_pairs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('sent_prop_src.txt', 'w')) as f:\n",
    "    for src in sent_prop_src:\n",
    "        f.write('\\t'.join([str(x) for x in src]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    with(open(f'sent_prop_pairs/sent_prop_pairs_{i}.txt', 'w')) as f:\n",
    "        for pair in sent_prop_pairs[i*len(sent_prop_pairs)//4:(i+1)*len(sent_prop_pairs)//4]:\n",
    "            f.write('\\t'.join(pair) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: sent_prop_pairs/ (stored 0%)\n",
      "  adding: sent_prop_pairs/sent_prop_pairs_1.txt (deflated 82%)\n",
      "  adding: sent_prop_pairs/sent_prop_pairs_4.txt (deflated 81%)\n",
      "  adding: sent_prop_pairs/sent_prop_pairs_2.txt (deflated 81%)\n",
      "  adding: sent_prop_pairs/sent_prop_pairs_3.txt (deflated 81%)\n",
      "  adding: sent_prop_pairs/sent_prop_pairs_0.txt (deflated 82%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r sent_prop_pairs.zip sent_prop_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('textbox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84fc63c8ae42b05f54f8c8e4c73411ce0404f059987aac7c448c556c45688d5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
